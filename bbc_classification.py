# -*- coding: utf-8 -*-
"""BBC Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VkCXYmb2a7KbOui_YHujEckzlSDek-F9

# **Imports & Drive mounting**
"""

import os
import string
string.punctuation
import re
import sys

import pandas as pd

import nltk
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
wordnet = nltk.stem.WordNetLemmatizer()
stopwords = nltk.corpus.stopwords.words('english')

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import FeatureUnion

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

from textblob import TextBlob

"""# **Read data**"""

folder = "./bbc"

dataset = []

for dir_name, dirs, files in os.walk(folder):
  for dir in dirs:
    dir_path = os.path.join(dir_name + "/" + dir)
    for file in os.listdir(dir_path):
      enc = 'iso-8859-15'
      f = open(os.path.join(dir_path + "/" + file), "r", encoding=enc)
      file_content = f.read()
      dataset.append((os.path.basename(dir_path), file_content))

print ("Length of dataset: ", len(dataset))

"""
# **Create a panda dataframe**
The list created in above block will be used to create a pandas data frame. The function `from_records` is used & custom column names are provided.
"""

labels=["category", "text"]
df = pd.DataFrame.from_records(dataset, columns=labels)

print ("************ Dataframe info ************")
print (df.info())
print ("****************************************\n")

print ("************ print dataframe ************")
print (df.head())
print ("*******************************************")

"""
# **Data Pre-processing**

The following steps are taken to cleanse the data:
1. Convert to lower case
2. Remove punctuations
3. Remove numbers
4. Remove line breaks
5. Remove stop words
6. Lemmatize words
"""

def processing(df):
  ## data cleansing
  df['text'] = df["text"].apply(lambda x: convert_lower(x))
  df['text'] = df["text"].apply(lambda x: remove_punctuation(x))
  df['text'] = df["text"].apply(lambda x: remove_numbers(x))
  df['text'] = df["text"].apply(lambda x: remove_line_breaks(x))
  df['text'] = df["text"].apply(lambda x: remove_stopwords(x))
  df['text'] = df["text"].apply(lambda x: lemmatize_word(x))

  return (df)

def convert_lower(text):
   return text.lower()

def remove_punctuation(text):
  output = "".join([i for i in text if i not in string.punctuation])
  return output

def remove_numbers(text):
  output = re.sub(r'[0-9]+', '', text)
  return output

def remove_line_breaks(text):
  return text.replace("\n", " ")

def remove_stopwords(text):
  words = word_tokenize(text)
  return [x for x in words if x not in stopwords]

def lemmatize_word(text):
  return " ".join([wordnet.lemmatize(word) for word in text])

processing(df)
print ("data cleaning done...")
print (df.head())

"""
# **Feature Engineering**
Here we introduce two more features for the text - 
1. Word count analysis : Total number of words in the text article
2. Sentiment analysis of the text article
"""

def addFeatures(df):
  ## word count in text
  df["words"] = df['text'].apply(lambda x: len(x.split(' ')))
  ## sentiment of the text
  df["sentiment"] = df["text"].apply(lambda x: TextBlob(x).sentiment.polarity)

addFeatures(df)

"""
# **Exploratory Data Analysis**
1. Graph to plot data distribution
2. Graph to plot historgram/density of sentiment analysis
3. Graph to plot histogram/density of word count analysis
4. Word cloud analysis for each category
"""

x = "category" 
fig, ax = plt.subplots()
fig.suptitle(x, fontsize=12)
df[x].reset_index().groupby(x).count().sort_values(by= 
       "index").plot(kind="barh", legend=False, 
        ax=ax).grid(axis='x')
plt.show()

x, y = "sentiment", "category"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df[y].unique():
    sns.distplot(df[df[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df[df[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df[y].unique())
ax[1].grid(True)
plt.show()

x, y = "words", "category"
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle(x, fontsize=12)
for i in df[y].unique():
    sns.distplot(df[df[y]==i][x], hist=True, kde=False, 
                 bins=10, hist_kws={"alpha":0.8}, 
                 axlabel="histogram", ax=ax[0])
    sns.distplot(df[df[y]==i][x], hist=False, kde=True, 
                 kde_kws={"shade":True}, axlabel="density",   
                 ax=ax[1])
ax[0].grid(True)
ax[0].legend(df[y].unique())
ax[1].grid(True)
plt.show()

df["tokenized_text"] = df["text"].apply(lambda x: word_tokenize(x))

# visualize word frequncy for politics news
print ("********************* Politics Word Cloud **************************")
pol = df[df['category'] == 'politics']
wordcloud = WordCloud(background_color="white", width = 1000, height = 500).generate(''.join(str(pol["tokenized_text"])))
plt.figure(figsize=(10,7))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()
print ("********************************************************************\n")

# visualize word frequncy for tech news
print ("********************** Tech Word Cloud ****************************")
pol = df[df['category'] == 'tech']
wordcloud = WordCloud(background_color="white", width = 1000, height = 500).generate(''.join(str(pol["tokenized_text"])))
plt.figure(figsize=(10,7))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()
print ("********************************************************************\n")

# visualize word frequncy for sport news
print ("********************** Sports Word Cloud **************************")
pol = df[df['category'] == 'sport']
wordcloud = WordCloud(background_color="white", width = 1000, height = 500).generate(''.join(str(pol["tokenized_text"])))
plt.figure(figsize=(10,7))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()
print ("*******************************************************************\n")

# visualize word frequncy for entertainment news
print ("******************* Entertainment Word Cloud *********************")
pol = df[df['category'] == 'entertainment']
wordcloud = WordCloud(background_color="white", width = 1000, height = 500).generate(''.join(str(pol["tokenized_text"])))
plt.figure(figsize=(10,7))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()
print ("******************************************************************\n")

# visualize word frequncy for business news
print ("*********************** Business Word Cloud ************************")
pol = df[df['category'] == 'business']
wordcloud = WordCloud(background_color="white", width = 1000, height = 500).generate(''.join(str(pol["tokenized_text"])))
plt.figure(figsize=(10,7))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()
print ("******************************************************************** \n")

"""
# **Model Training & Evaluation**
Defining some helper classes
"""

## Transformer to select text based column from dataframe to do more transformations
class TextSelector(BaseEstimator, TransformerMixin):
    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[self.key]
    
## Transformer to select numeric based column from dataframe to do more transformations
class NumberSelector(BaseEstimator, TransformerMixin):
    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[[self.key]]

"""Test/Train split in the ration 80/20"""

features= [c for c in df.columns.values if c  not in ['category']]
target = 'category'

X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)

"""
1. Create a sklearn pipeline
2. feature - tfidf pipeline (based on word frequency)
3. words pipeline scaled using minmaxscaler
4. sentiment pipeline scaled using minmaxscaler
5. feature union
"""

## TFIDF
text = Pipeline([
                ('selector', TextSelector(key='text')),
                ('tfidf', TfidfVectorizer(ngram_range=(1,3))),
            ])

text.fit_transform(X_train)

## NUMBER OF WORDS
words =  Pipeline([
                ('selector', NumberSelector(key='words')),
                ('standard', MinMaxScaler())
            ])

words.fit_transform(X_train)

## SENTIMENT
sentiment =  Pipeline([
                ('selector', NumberSelector(key='sentiment')),
                ('standard', MinMaxScaler())
            ])

sentiment.fit_transform(X_train)

## Combine all features using featureunion
all_features = FeatureUnion([
                      ('text', text), 
                      ('sentiment', sentiment),
                      ('words', words),
                    ])

feature_processing = Pipeline([('all_features', all_features)])
feature_processing.fit_transform(X_train)

"""Ultimate Pipeline creation with feature selection, Model training & Model Evaluation"""

pipeline = Pipeline([
    ('features', all_features),
    ('chi', SelectKBest(chi2, k=1000)),
    ('clf', LinearSVC(C=1.0, penalty='l1', max_iter=3000, dual=False)),
])

print ("training model using Linear SVC....")
model = pipeline.fit(X_train, y_train)
print ("model trained...")

print ("predicting test set using model...")
preds = pipeline.predict(X_test)

precision=precision_score(preds, y_test, average='macro')
recall=recall_score(preds, y_test, average='macro')
f1=f1_score(preds, y_test, average='macro')
accuracy=accuracy_score(preds, y_test)

print ("prediction analysis: ")
print ("Accuracy: "+str(round(accuracy,3)))
print ("Precision (macro-average): "+str(round(precision,3)))
print ("Recall (macro-average): "+str(round(recall,3)))
print ("F1-Score (macro-average): "+str(round(f1,3)))

"""Predictions on some random article headlines from bbc"""

samples = [
  "Hooli stock price soared after a dip in PiedPiper revenue growth.",
  "Captain Tsubasa scores a magnificent goal for the Japanese team.",
  "Merryweather mercenaries are sent on another mission, as government oversight groups call for new sanctions.",
  "Beyonc√© releases a new album, tops the charts in all of south-east Asia!",
  "Apple uses best in class technology"
]

new_test_data = pd.DataFrame(samples, columns=['text'])
processing(new_test_data)
addFeatures(new_test_data)

print(pipeline.predict(new_test_data))